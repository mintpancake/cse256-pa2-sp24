(cse256) PS D:\Local\Workspace\CSE256\cse256-pa2-sp24> python -u "d:\Local\Workspace\CSE256\cse256-pa2-sp24\PA2_code\main.py"
Loading data and creating tokenizer ...
Vocabulary size is 5755
Epoch: 0, Train Accuracy: 46.41491395793499, Test Accuracy: 37.06666666666667
Epoch: 1, Train Accuracy: 52.48565965583174, Test Accuracy: 44.93333333333333
Epoch: 2, Train Accuracy: 56.978967495219884, Test Accuracy: 50.13333333333333
Epoch: 3, Train Accuracy: 66.87380497131932, Test Accuracy: 56.266666666666666
Epoch: 4, Train Accuracy: 76.33843212237093, Test Accuracy: 65.33333333333333
Epoch: 5, Train Accuracy: 81.83556405353728, Test Accuracy: 71.06666666666666
Epoch: 6, Train Accuracy: 87.8585086042065, Test Accuracy: 75.46666666666667
Epoch: 7, Train Accuracy: 91.49139579349904, Test Accuracy: 79.2
Epoch: 8, Train Accuracy: 94.93307839388146, Test Accuracy: 83.86666666666666
Epoch: 9, Train Accuracy: 95.26768642447419, Test Accuracy: 83.2
Epoch: 10, Train Accuracy: 93.49904397705545, Test Accuracy: 81.46666666666667
Epoch: 11, Train Accuracy: 97.56214149139579, Test Accuracy: 84.93333333333334
Epoch: 12, Train Accuracy: 97.51434034416826, Test Accuracy: 85.33333333333333
Epoch: 13, Train Accuracy: 98.61376673040154, Test Accuracy: 85.33333333333333
Epoch: 14, Train Accuracy: 99.1395793499044, Test Accuracy: 86.26666666666667
Input tensor shape: torch.Size([1, 32])
Number of attention maps: 8
attn_maps/attn_map_Encoder_1.png
attn_maps/attn_map_Encoder_2.png
attn_maps/attn_map_Encoder_3.png
attn_maps/attn_map_Encoder_4.png
attn_maps/attn_map_Encoder_5.png
attn_maps/attn_map_Encoder_6.png
attn_maps/attn_map_Encoder_7.png
attn_maps/attn_map_Encoder_8.png
Input tensor shape: torch.Size([1, 32])
Number of attention maps: 8
attn_maps/attn_map_Encoder_1.png
attn_maps/attn_map_Encoder_2.png
attn_maps/attn_map_Encoder_3.png
attn_maps/attn_map_Encoder_4.png
attn_maps/attn_map_Encoder_5.png
attn_maps/attn_map_Encoder_6.png
attn_maps/attn_map_Encoder_7.png
attn_maps/attn_map_Encoder_8.png
Iteration: 99, Train Perplexity: 579.6377563476562, Test Perplexity hbush: 709.1610107421875, Test Perplexity obama: 680.533935546875, Test Perplexity wbush: 777.2838134765625
Iteration: 199, Train Perplexity: 393.8780517578125, Test Perplexity hbush: 548.2493896484375, Test Perplexity obama: 521.4069213867188, Test Perplexity wbush: 614.6256103515625
Iteration: 299, Train Perplexity: 268.3271484375, Test Perplexity hbush: 450.506103515625, Test Perplexity obama: 416.6581726074219, Test Perplexity wbush: 512.3653564453125
Iteration: 399, Train Perplexity: 195.18675231933594, Test Perplexity hbush: 403.4720764160156, Test Perplexity obama: 369.88470458984375, Test Perplexity wbush: 471.4269104003906
Iteration: 499, Train Perplexity: 152.8769073486328, Test Perplexity hbush: 390.1326904296875, Test Perplexity obama: 349.01202392578125, Test Perplexity wbush: 467.079345703125
Input tensor shape: torch.Size([1, 32])
Number of attention maps: 8
attn_maps/attn_map_Decoder_1.png
attn_maps/attn_map_Decoder_2.png
attn_maps/attn_map_Decoder_3.png
attn_maps/attn_map_Decoder_4.png
d:\Local\Workspace\CSE256\cse256-pa2-sp24\PA2_code\utilities.py:50: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots()
attn_maps/attn_map_Decoder_5.png
attn_maps/attn_map_Decoder_6.png
attn_maps/attn_map_Decoder_7.png
attn_maps/attn_map_Decoder_8.png
Input tensor shape: torch.Size([1, 32])
Number of attention maps: 8
attn_maps/attn_map_Decoder_1.png
attn_maps/attn_map_Decoder_2.png
attn_maps/attn_map_Decoder_3.png
attn_maps/attn_map_Decoder_4.png
attn_maps/attn_map_Decoder_5.png
attn_maps/attn_map_Decoder_6.png
attn_maps/attn_map_Decoder_7.png
attn_maps/attn_map_Decoder_8.png