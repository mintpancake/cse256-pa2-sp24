(cse256) PS D:\Local\Workspace\CSE256\cse256-pa2-sp24> python -u "d:\Local\Workspace\CSE256\cse256-pa2-sp24\PA2_code\main.py" part3
Loading data and creating tokenizer ...
[00:00:00] Pre-processing sequences       █████████████████████████████████████████████████████████████████████████████████ 0        /        0[00:00:00] Tokenize words                 █████████████████████████████████████████████████████████████████████████████████ 5572     /     5572
[00:00:00] Count pairs                    █████████████████████████████████████████████████████████████████████████████████ 5572     /     5572
[00:00:00] Compute merges                 █████████████████████████████████████████████████████████████████████████████████ 6545     /     6545
Vocabulary size is 6627
Epoch: 0, Train Accuracy: 44.64627151051625, Test Accuracy: 33.333333333333336
Epoch: 1, Train Accuracy: 52.58126195028681, Test Accuracy: 43.333333333333336
Epoch: 2, Train Accuracy: 63.718929254302104, Test Accuracy: 60.4
Epoch: 3, Train Accuracy: 70.26768642447419, Test Accuracy: 64.13333333333334
Epoch: 4, Train Accuracy: 72.65774378585085, Test Accuracy: 65.73333333333333
Epoch: 5, Train Accuracy: 81.83556405353728, Test Accuracy: 70.53333333333333
Epoch: 6, Train Accuracy: 85.94646271510516, Test Accuracy: 73.86666666666666
Epoch: 7, Train Accuracy: 90.43977055449331, Test Accuracy: 79.6
Epoch: 8, Train Accuracy: 93.59464627151051, Test Accuracy: 83.46666666666667
Epoch: 9, Train Accuracy: 93.97705544933078, Test Accuracy: 83.73333333333333
Epoch: 10, Train Accuracy: 95.50669216061185, Test Accuracy: 84.4
Epoch: 11, Train Accuracy: 96.12810707456978, Test Accuracy: 84.26666666666667
Epoch: 12, Train Accuracy: 96.65391969407266, Test Accuracy: 85.73333333333333
Epoch: 13, Train Accuracy: 96.98852772466539, Test Accuracy: 85.73333333333333
Epoch: 14, Train Accuracy: 97.131931166348, Test Accuracy: 86.0
Epoch: 15, Train Accuracy: 97.32313575525812, Test Accuracy: 86.13333333333334
Epoch: 16, Train Accuracy: 97.7055449330784, Test Accuracy: 86.53333333333333
Epoch: 17, Train Accuracy: 98.04015296367113, Test Accuracy: 86.66666666666667
Epoch: 18, Train Accuracy: 97.99235181644359, Test Accuracy: 86.93333333333334
Epoch: 19, Train Accuracy: 98.27915869980879, Test Accuracy: 86.93333333333334
Input tensor shape: torch.Size([1, 32])
Number of attention maps: 8
attn_maps/attn_map_Encoder_1.png
attn_maps/attn_map_Encoder_2.png
attn_maps/attn_map_Encoder_3.png
attn_maps/attn_map_Encoder_4.png
attn_maps/attn_map_Encoder_5.png
attn_maps/attn_map_Encoder_6.png
attn_maps/attn_map_Encoder_7.png
attn_maps/attn_map_Encoder_8.png
Input tensor shape: torch.Size([1, 32])
Number of attention maps: 8
attn_maps/attn_map_Encoder_1.png
attn_maps/attn_map_Encoder_2.png
attn_maps/attn_map_Encoder_3.png
attn_maps/attn_map_Encoder_4.png
attn_maps/attn_map_Encoder_5.png
attn_maps/attn_map_Encoder_6.png
attn_maps/attn_map_Encoder_7.png
attn_maps/attn_map_Encoder_8.png
Iteration: 99, Train Perplexity: 491.83343505859375, Test Perplexity hbush: 693.9244995117188, Test Perplexity obama: 658.2604370117188, Test Perplexity wbush: 797.8609619140625
Iteration: 199, Train Perplexity: 305.41619873046875, Test Perplexity hbush: 543.875244140625, Test Perplexity obama: 499.1941223144531, Test Perplexity wbush: 649.261962890625
Iteration: 299, Train Perplexity: 205.44093322753906, Test Perplexity hbush: 455.12109375, Test Perplexity obama: 418.8447570800781, Test Perplexity wbush: 545.4647216796875
Iteration: 399, Train Perplexity: 152.57460021972656, Test Perplexity hbush: 431.3528747558594, Test Perplexity obama: 387.7857666015625, Test Perplexity wbush: 526.5686645507812
Iteration: 499, Train Perplexity: 117.08658599853516, Test Perplexity hbush: 426.68682861328125, Test Perplexity obama: 380.61456298828125, Test Perplexity wbush: 525.9520874023438
Input tensor shape: torch.Size([1, 32])
Number of attention maps: 8
attn_maps/attn_map_Decoder_1.png
attn_maps/attn_map_Decoder_2.png
attn_maps/attn_map_Decoder_3.png
attn_maps/attn_map_Decoder_4.png
d:\Local\Workspace\CSE256\cse256-pa2-sp24\PA2_code\utilities.py:50: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots()
attn_maps/attn_map_Decoder_5.png
attn_maps/attn_map_Decoder_6.png
attn_maps/attn_map_Decoder_7.png
attn_maps/attn_map_Decoder_8.png
Input tensor shape: torch.Size([1, 32])
Number of attention maps: 8
attn_maps/attn_map_Decoder_1.png
attn_maps/attn_map_Decoder_2.png
attn_maps/attn_map_Decoder_3.png
attn_maps/attn_map_Decoder_4.png
attn_maps/attn_map_Decoder_5.png
attn_maps/attn_map_Decoder_6.png
attn_maps/attn_map_Decoder_7.png
attn_maps/attn_map_Decoder_8.png